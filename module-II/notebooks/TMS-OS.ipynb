{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMS-OS (Tiered Multi-Sensor: Optical & SAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import sigmaclip\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploring Sentinel-2 time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sentinel-2 data\n",
    "s2 = pd.read_csv(\"../data/s2-sirindhorn.csv\", parse_dates=['date']).set_index('date')\n",
    "s2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cloud cover in % over the ROI\n",
    "s2['cloud_percentage'] = (s2['cloud_area']*100)/(s2['water_area_clustering']+s2['non_water_area_clustering']+s2['cloud_area'])\n",
    "s2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    row_heights=[0.8, 0.2],\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing = 0.05\n",
    ")\n",
    "\n",
    "# Raw Surface Area\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = s2.index,\n",
    "    y = s2['water_area_clustering'],\n",
    "    name = 'Raw Reservoir Surface Area',\n",
    "    mode = 'lines+markers'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Cloud Corrected Surface Area\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = s2.index,\n",
    "    y = s2['water_area'],\n",
    "    name = 'Cloud-Corrected Reservoir Surface Area',\n",
    "    mode = 'lines+markers'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Cloud\n",
    "fig.add_trace(go.Bar(\n",
    "    x = s2.index,\n",
    "    y = s2['cloud_percentage'],\n",
    "    name = 'Cloud Cover (%)',\n",
    "    marker = dict(\n",
    "        color = 'red',\n",
    "    )\n",
    "), row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(                                         # title\n",
    "        text='Reservoir Surface Areas - Sentinel-2',\n",
    "        xanchor='center',\n",
    "        x=0.5\n",
    "    ),\n",
    "    legend=dict(                                        # legend\n",
    "        orientation = 'h',\n",
    "        yanchor='top',\n",
    "        y=-0.08,\n",
    "        xanchor='right',\n",
    "        x=1.0,\n",
    "        bordercolor=\"grey\",\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    margin=dict(l=20, r=20, t=60, b=20)                 # margins\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_There are a lot of `-1` values, what are those??_\n",
    "- When the cloud cover is >90% the script returns `-1` as a fill value. Essentially, we don't have a data point there.\n",
    "\n",
    "Let's remove these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all the -1 values as np.nan (Not-A-Number)\n",
    "s2.loc[s2['cloud_percentage']>90, ['water_area_clustering', 'non_water_area_clustering', 'cloud_area', 'water_area']] = np.nan\n",
    "\n",
    "# drop all the np.nan values\n",
    "s2.dropna(inplace=True)\n",
    "\n",
    "s2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    row_heights=[0.8, 0.2],\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing = 0.05\n",
    ")\n",
    "\n",
    "# Raw Surface Area\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = s2.index,\n",
    "    y = s2['water_area_clustering'],\n",
    "    name = 'Raw Reservoir Surface Area',\n",
    "    mode = 'lines+markers'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Cloud Corrected Surface Area\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = s2.index,\n",
    "    y = s2['water_area'],\n",
    "    name = 'Cloud-Corrected Reservoir Surface Area',\n",
    "    mode = 'lines+markers'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Cloud\n",
    "fig.add_trace(go.Bar(\n",
    "    x = s2.index,\n",
    "    y = s2['cloud_percentage'],\n",
    "    name = 'Cloud Cover (%)',\n",
    "    marker = dict(\n",
    "        color = 'red',\n",
    "    )\n",
    "), row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(                                         # title\n",
    "        text='Reservoir Surface Areas - Sentinel-2',\n",
    "        xanchor='center',\n",
    "        x=0.5\n",
    "    ),\n",
    "    legend=dict(                                        # legend\n",
    "        orientation = 'h',\n",
    "        yanchor='top',\n",
    "        y=-0.08,\n",
    "        xanchor='right',\n",
    "        x=1.0,\n",
    "        bordercolor=\"grey\",\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    margin=dict(l=20, r=20, t=60, b=20)                 # margins\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the sudden drops in surface areas, which usually occur during high cloud cover conditions, but may also happen during cloud-free days. These sudden drops (> 100 sq. km.) followed by sudden rise of similar magnitude in a span of 5-10 days aren't representative of the true behavior of the reservoirs - these are not actual signals. \n",
    "\n",
    "These artifacts can occur due to the automatic nature of the clustering algorithm ([Cascade simple K-Means clustering](https://developers.google.com/earth-engine/apidocs/ee-clusterer-wekacascadekmeans)). In the K-Means clustering algorithm, you'd have to specify the number of clusters ($K$) to form, before performing the clustering. Specifying a hard-coded value for $K$ can (1) be difficult, and (2) create additional issues, especially when several reservoirs are to be mapped at once (such as RAT-Mekong). Moreover, in case of a highly dynamic reservoir, where the area may change drastically during dry and wet seasons, the number of distinct features that can appear as a \"cluster\" may vary as well. Due to such reasons, it is recommended to use an automatic scheme of choosing the value of $K$, which, in this case, is done using [the Calinski-Harabasz criterion](https://www.tandfonline.com/doi/abs/10.1080/03610927408827101).\n",
    "\n",
    "The limitation with this method, however, is that the Calinski-Harabasz crierion can sometimes choose a value of $K$ where the cluster representing water pixels gets divided into multiple clusters, during challenging scenarios. Such challenging scenarios can occur due to unmasked cloud cover, sediment-laden the water, intermittent vegetation, and other artifacts of processing done by the Satellite data provider.\n",
    "\n",
    "**These erroneous values get corrected in the subsequent steps of TMS-OS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's load the datasets and plot them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOUD_THRESHOLD = 90\n",
    "L8_TEMPORAL_RESOLUTION = 16\n",
    "S2_TEMPORAL_RESOLUTION = 5\n",
    "S1_TEMPORAL_RESOLUTION = 12\n",
    "\n",
    "# LANDSAT - 8\n",
    "l8df = pd.read_csv('../data/l8-sirindhorn.csv', parse_dates=['mosaic_enddate']).rename({\n",
    "            'mosaic_enddate': 'date',\n",
    "            'water_area_cordeiro': 'water_area_uncorrected',\n",
    "            'non_water_area_cordeiro': 'non_water_area', \n",
    "            'corrected_area_cordeiro': 'water_area_corrected'\n",
    "            }, axis=1).set_index('date')\n",
    "l8df = l8df[['water_area_uncorrected', 'non_water_area', 'cloud_area', 'water_area_corrected']]\n",
    "l8df['cloud_percent'] = l8df['cloud_area']*100/(l8df['water_area_uncorrected']+l8df['non_water_area']+l8df['cloud_area'])\n",
    "l8df.replace(-1, np.nan, inplace=True)\n",
    "\n",
    "# QUALITY_DESCRIPTION\n",
    "#   0: Good, not interpolated either due to missing data or high clouds\n",
    "#   1: Poor, interpolated either due to high clouds\n",
    "#   2: Poor, interpolated either due to missing data\n",
    "l8df.loc[:, \"QUALITY_DESCRIPTION\"] = 0\n",
    "l8df.loc[l8df['cloud_percent']>=CLOUD_THRESHOLD, (\"water_area_uncorrected\", \"non_water_area\", \"water_area_corrected\")] = np.nan\n",
    "l8df.loc[l8df['cloud_percent']>=CLOUD_THRESHOLD, \"QUALITY_DESCRIPTION\"] = 1\n",
    "\n",
    "# in some cases l8df may have duplicated rows (with same values) that have to be removed\n",
    "if l8df.index.duplicated().sum() > 0:\n",
    "    print(\"Duplicated labels, deleting\")\n",
    "    l8df = l8df[~l8df.index.duplicated(keep='last')]\n",
    "\n",
    "# Fill in the gaps in l8df created due to high cloud cover with np.nan values\n",
    "l8df_interpolated = l8df.reindex(pd.date_range(l8df.index[0], l8df.index[-1], freq=f'{L8_TEMPORAL_RESOLUTION}D'))\n",
    "l8df_interpolated.loc[np.isnan(l8df_interpolated[\"QUALITY_DESCRIPTION\"]), \"QUALITY_DESCRIPTION\"] = 2\n",
    "l8df_interpolated.loc[np.isnan(l8df_interpolated['cloud_area']), 'cloud_area'] = max(l8df['cloud_area'])\n",
    "l8df_interpolated.loc[np.isnan(l8df_interpolated['cloud_percent']), 'cloud_percent'] = 100\n",
    "l8df_interpolated.loc[np.isnan(l8df_interpolated['non_water_area']), 'non_water_area'] = 0\n",
    "l8df_interpolated.loc[np.isnan(l8df_interpolated['water_area_uncorrected']), 'water_area_uncorrected'] = 0\n",
    "\n",
    "# Interpolate bad data\n",
    "l8df_interpolated.loc[:, \"water_area_corrected\"] = l8df_interpolated.loc[:, \"water_area_corrected\"].interpolate(method=\"linear\", limit_direction=\"forward\")\n",
    "\n",
    "\n",
    "# Read in Sentinel-2 data\n",
    "s2df = pd.read_csv('../data/s2-sirindhorn.csv', parse_dates=['date']).set_index('date')\n",
    "s2df = s2df[['water_area_clustering', 'non_water_area_clustering', 'cloud_area', 'water_area']]\n",
    "s2df['cloud_percent'] = s2df['cloud_area']*100/(s2df['water_area_clustering']+s2df['non_water_area_clustering']+s2df['cloud_area'])\n",
    "s2df.replace(-1, np.nan, inplace=True)\n",
    "s2df.loc[s2df['cloud_percent']>=CLOUD_THRESHOLD, (\"water_area_clustering\", \"non_water_area_clustering\", \"water_area\")] = np.nan\n",
    "\n",
    "# QUALITY_DESCRIPTION\n",
    "#   0: Good, not interpolated either due to missing data or high clouds\n",
    "#   1: Poor, interpolated either due to high clouds\n",
    "#   2: Poor, interpolated either due to missing data\n",
    "s2df.loc[:, \"QUALITY_DESCRIPTION\"] = 0\n",
    "s2df.loc[s2df['cloud_percent']>=CLOUD_THRESHOLD, \"QUALITY_DESCRIPTION\"] = 1\n",
    "\n",
    "# in some cases s2df may have duplicated rows (with same values) that have to be removed\n",
    "if s2df.index.duplicated().sum() > 0:\n",
    "    print(\"Duplicated labels, deleting\")\n",
    "    s2df = s2df[~s2df.index.duplicated(keep='last')]\n",
    "\n",
    "# Fill in the gaps in s2df created due to high cloud cover with np.nan values\n",
    "s2df_interpolated = s2df.reindex(pd.date_range(s2df.index[0], s2df.index[-1], freq=f'{S2_TEMPORAL_RESOLUTION}D'))\n",
    "s2df_interpolated.loc[np.isnan(s2df_interpolated[\"QUALITY_DESCRIPTION\"]), \"QUALITY_DESCRIPTION\"] = 2\n",
    "s2df_interpolated.loc[np.isnan(s2df_interpolated['cloud_area']), 'cloud_area'] = max(s2df['cloud_area'])\n",
    "s2df_interpolated.loc[np.isnan(s2df_interpolated['cloud_percent']), 'cloud_percent'] = 100\n",
    "s2df_interpolated.loc[np.isnan(s2df_interpolated['non_water_area_clustering']), 'non_water_area_clustering'] = 0\n",
    "s2df_interpolated.loc[np.isnan(s2df_interpolated['water_area_clustering']), 'water_area_clustering'] = 0\n",
    "\n",
    "# Interpolate bad data\n",
    "s2df_interpolated.loc[:, \"water_area\"] = s2df_interpolated.loc[:, \"water_area\"].interpolate(method=\"linear\", limit_direction=\"forward\")\n",
    "\n",
    "\n",
    "# Sentinel - 1\n",
    "# Read in Sentinel-1 data\n",
    "sar = pd.read_csv('../data/sar-sirindhorn.csv', parse_dates=['time']).rename({'time': 'date'}, axis=1)\n",
    "sar['date'] = sar['date'].apply(lambda d: np.datetime64(d.strftime('%Y-%m-%d')))\n",
    "sar.set_index('date', inplace=True)\n",
    "sar.sort_index(inplace=True)\n",
    "sar = sar.loc['2019-01-01':, :]\n",
    "# in some cases sar may have duplicated rows (with same values) that have to be removed\n",
    "if sar.index.duplicated().sum() > 0:\n",
    "    sar = sar[~sar.index.duplicated(keep='last')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging, Filtering and Correcting based on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the functions\n",
    "# https://gist.github.com/pritamd47/e7ddc49f25ae7f1b06c201f0a8b98348\n",
    "# Clip time-series\n",
    "def clip_ts(*tss, which='left'):\n",
    "    \"\"\"Clips multiple time-series to align them temporally\n",
    "\n",
    "    Args:\n",
    "        which (str, optional): Defines which direction the clipping will be performed. \n",
    "                               'left' will clip the time-series only on the left side of the \n",
    "                               unaligned time-serieses, and leave the right-side untouched, and \n",
    "                               _vice versa_. Defaults to 'left'. Options can be: 'left', 'right' \n",
    "                               or 'both'\n",
    "\n",
    "    Returns:\n",
    "        lists: returns the time-series as an unpacked list in the same order that they were passed\n",
    "    \"\"\"\n",
    "    mint = max([min(ts.index) for ts in tss])\n",
    "    maxt = min([max(ts.index) for ts in tss])\n",
    "\n",
    "    if which == 'both':\n",
    "        clipped_tss = [ts.loc[(ts.index>=mint)&(ts.index<=maxt)] for ts in tss]\n",
    "    elif which == 'left':\n",
    "        clipped_tss = [ts.loc[ts.index>=mint] for ts in tss]\n",
    "    elif which == 'right':\n",
    "        clipped_tss = [ts.loc[ts.index<=maxt] for ts in tss]\n",
    "    else:\n",
    "        raise Exception(f'Unknown option passed: {which}, expected \"left\", \"right\" or \"both\"./')\n",
    "\n",
    "    return clipped_tss\n",
    "\n",
    "def deviation_from_sar(optical_areas, sar_areas, DEVIATION_THRESHOLD = 20, LOW_STD_LIM=2, HIGH_STD_LIM=2):\n",
    "    \"\"\"Filter out points based on deviations from SAR reported areas after correcting for bias in SAR water areas. Remove NaNs beforehand.\n",
    "\n",
    "    Args:\n",
    "        optical_areas (pd.Series): Time-series of areas obtained using an optical sensor (S2, L8, etc) on which the filtering will be applied. Must have `pd.DatetimeIndex` and corresponding areas in a column named `area`.\n",
    "        sar_areas (pd.Series): Time-series of S1 surface areas. Must have `pd.DatetimeIndex` and corresponding areas in a column named `area`.\n",
    "        DEVIATION_THRESHOLD (number): (Default: 20 [sq. km.]) Theshold of deviation from bias corrected SAR reported \n",
    "        LOW_STD_LIM (number): (Default: 2) Lower limit of standard deviations to use for clipping the deviations, required for calculating the bias.\n",
    "        HIGH_STD_LIM (number): (Default: 2) Upper limit of standard deviations to use for clipping the deviations, required for calculating the bias.\n",
    "    \"\"\"\n",
    "    # convert to dataframes under the hood\n",
    "    optical_areas = optical_areas.to_frame()\n",
    "    sar_areas = sar_areas.to_frame()\n",
    "\n",
    "    xs = sar_areas.index.view(np.int64)//10**9  # Convert datetime to seconds from epoch\n",
    "    ys = sar_areas['area']\n",
    "    sar_area_func = interp1d(xs, ys, bounds_error=False)\n",
    "    \n",
    "    # Interpolate and calculate the sar reported areas according to the optical sensor's observation dates\n",
    "    sar_sarea_interpolated = sar_area_func(optical_areas.index.view(np.int64)//10**9)\n",
    "    deviations = optical_areas['area'] - sar_sarea_interpolated\n",
    "\n",
    "    clipped = sigmaclip(deviations.dropna(), low=LOW_STD_LIM, high=HIGH_STD_LIM)\n",
    "    bias = np.median(clipped.clipped)\n",
    "\n",
    "    optical_areas['normalized_dev'] = deviations - bias\n",
    "    optical_areas['flagged'] = False\n",
    "    optical_areas.loc[np.abs(optical_areas['normalized_dev']) > DEVIATION_THRESHOLD, 'flagged'] = True\n",
    "    optical_areas.loc[optical_areas['flagged'], 'area'] = np.nan\n",
    "\n",
    "    return optical_areas['area']\n",
    "\n",
    "# helper functions\n",
    "def sar_trend(d1, d2, sar):\n",
    "    subset = sar['area'].resample('1D').interpolate('linear')\n",
    "    subset = subset.loc[d1:d2]\n",
    "    if len(subset) == 0:\n",
    "        trend = np.nan\n",
    "    else:\n",
    "        trend = (subset.iloc[-1]-subset.iloc[0])/((np.datetime64(d2)-np.datetime64(d1))/np.timedelta64(1, 'D'))\n",
    "    return trend\n",
    "\n",
    "def backcalculate(areas, trends, who_needs_correcting):\n",
    "    # identify the first reliable point\n",
    "    unreliable_pts_at_the_beginning = len(who_needs_correcting[:who_needs_correcting.idxmin()])-1\n",
    "    corrected_areas = [np.nan] * unreliable_pts_at_the_beginning\n",
    "    corrected_areas.append(areas.iloc[unreliable_pts_at_the_beginning+1])\n",
    "\n",
    "    # # calculate previous points\n",
    "    # for area, correction_required, trend in zip(areas[unreliable_pts_at_the_beginning::-1], who_needs_correcting[unreliable_pts_at_the_beginning::-1], trends[unreliable_pts_at_the_beginning::-1]):\n",
    "    #     print(area, correction_required, trend)\n",
    "    \n",
    "    for area, correction_required, trend in zip(areas[unreliable_pts_at_the_beginning+1:], who_needs_correcting[unreliable_pts_at_the_beginning+1:], trends[unreliable_pts_at_the_beginning+1:]):\n",
    "        if not correction_required:\n",
    "            corrected_areas.append(area)\n",
    "        else:\n",
    "            corrected_area = corrected_areas[-1] + trend\n",
    "            corrected_areas.append(corrected_area)\n",
    "    \n",
    "    return corrected_areas\n",
    "\n",
    "def deviation_correction(area, DEVIATION_THRESHOLD, AREA_COL_NAME='area'):\n",
    "    inner_area = area.copy()\n",
    "\n",
    "    inner_area.loc[:, 'deviation'] = np.abs(inner_area['trend']-inner_area['sar_trend'])\n",
    "\n",
    "    inner_area.loc[:, 'erroneous'] = inner_area['deviation'] > DEVIATION_THRESHOLD\n",
    "\n",
    "    inner_area.loc[:, 'corrected_trend'] = inner_area['trend']\n",
    "    inner_area.loc[inner_area['erroneous'], 'corrected_trend'] = inner_area['sar_trend']\n",
    "\n",
    "    areas = backcalculate(inner_area[AREA_COL_NAME], inner_area['corrected_trend'], inner_area['erroneous'])\n",
    "    inner_area[AREA_COL_NAME] = areas\n",
    "\n",
    "    return inner_area\n",
    "\n",
    "def sign_based_correction(area, AREA_COL_NAME='corrected_areas_1', TREND_COL_NAME='corrected_trend_1'):\n",
    "    inner_area = area.copy()\n",
    "    inner_area['sign_based_correction_reqd'] = (inner_area['trend']<0)&(inner_area['sar_trend']>0)|(inner_area['trend']>0)&(inner_area['sar_trend']<0)\n",
    "    inner_area.loc[:, 'corrected_trend'] = inner_area[TREND_COL_NAME]\n",
    "    inner_area.loc[inner_area['sign_based_correction_reqd'], 'corrected_trend'] = inner_area['sar_trend']\n",
    "\n",
    "    inner_area['area'] = backcalculate(inner_area[AREA_COL_NAME], inner_area['corrected_trend'], inner_area['sign_based_correction_reqd'])\n",
    "\n",
    "    return inner_area\n",
    "\n",
    "def filled_by_trend(filtered_area, sar_trend, days_passed) -> pd.Series:\n",
    "    \"\"\"Fills in `np.nan` values of optically obtained surface area time series using SAR based time-series.\n",
    "\n",
    "    Args:\n",
    "        filtered_area (pd.Series): Optical sensor based surface areas containing `np.nan` values that will be filled in.\n",
    "        sar_trend (pd.Series): SAR based surface area trends.\n",
    "        days_passed (pd.Series): Days sicne last observation of optical sensor observed surface areas.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Filled nan values\n",
    "    \"\"\"\n",
    "    filled = [filtered_area.iloc[0]]\n",
    "    for i in range(1, len(filtered_area)):\n",
    "        if np.isnan(filtered_area.iloc[i]):\n",
    "            a = filled[-1] + sar_trend.iloc[i] * days_passed.iloc[i]\n",
    "            filled.append(a)\n",
    "        else:\n",
    "            filled.append(filtered_area.iloc[i])\n",
    "    \n",
    "    return pd.Series(filled, dtype=float, name='filled_area', index=filtered_area.index)\n",
    "\n",
    "# Trend based correction function\n",
    "def trend_based_correction(area, sar, AREA_DEVIATION_THRESHOLD=25, TREND_DEVIATION_THRESHOLD = 10):\n",
    "    \"\"\"Apply trend based correction on a time-series\n",
    "\n",
    "    Args:\n",
    "        area (pd.DataFrame): Pandas dataframe containing date as pd.DatetimeIndex and areas in column named `area`\n",
    "        sar (pd.DataFrame): Pandas dataframe containing surface area time-series obtained from Sentinel-1 (SAR). Same format as `area`\n",
    "        AREA_DEVIATION_THRESHOLD (number): (Default: 25) Threshold value of deviation of optically derived areas from SAR derived areas fro filtering.\n",
    "        TREND_DEVIATION_THRESHOLD (number): (Default: 10) Threshold value of deviation in trend above which the observation is marked as erroneous and the correction step is applied\n",
    "    \"\"\"\n",
    "\n",
    "    area['filtered_area'] = deviation_from_sar(area['area'], sar['area'], AREA_DEVIATION_THRESHOLD)\n",
    "    area.rename({'area': 'unfiltered_area'}, axis=1, inplace=True)\n",
    "    # area.rename({'filtered_area': 'area'}, axis=1, inplace=True)\n",
    "    \n",
    "    area_filtered = area.dropna(subset=['filtered_area'])\n",
    "\n",
    "    area_filtered.loc[:, 'days_passed'] = area_filtered.index.to_series().diff().dt.days\n",
    "    area_filtered.loc[:, 'trend'] = area_filtered['filtered_area'].diff()/area_filtered['days_passed']\n",
    "\n",
    "    sar, area_filtered = clip_ts(sar, area_filtered, which='left')\n",
    "    # sometimes the sar time-series has duplicate values which have to be removed\n",
    "    sar = sar[~sar.index.duplicated(keep='first')]   # https://stackoverflow.com/a/34297689/4091712\n",
    "    trend_generator = lambda arg: sar_trend(arg.index[0], arg.index[-1], sar)\n",
    "\n",
    "    area_filtered.loc[:, 'sar_trend'] = area_filtered['filtered_area'].rolling(2, min_periods=0).apply(trend_generator)\n",
    "\n",
    "    deviation_correction_results = deviation_correction(area_filtered, TREND_DEVIATION_THRESHOLD, AREA_COL_NAME='filtered_area')\n",
    "    area_filtered['corrected_areas_1'] = deviation_correction_results['filtered_area']\n",
    "    area_filtered['corrected_trend_1'] = deviation_correction_results['corrected_trend']\n",
    "    \n",
    "    area['corrected_areas_1'] = area_filtered['corrected_areas_1']\n",
    "    area['corrected_trend_1'] = area_filtered['corrected_trend_1']\n",
    "\n",
    "    area.loc[:, 'sar_trend'] = area['unfiltered_area'].rolling(2, min_periods=0).apply(trend_generator)\n",
    "    area.loc[:, 'days_passed'] = area.index.to_series().diff().dt.days\n",
    "    \n",
    "    area, sar = clip_ts(area, sar, which=\"left\")\n",
    "    first_non_nan = area['corrected_areas_1'].first_valid_index()\n",
    "    area = area.loc[first_non_nan:, :]\n",
    "\n",
    "    # fill na based on trends\n",
    "    area['filled_area'] = filled_by_trend(area['corrected_areas_1'], area['sar_trend'], area['days_passed'])\n",
    "\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine opticals into one dataframes\n",
    "l8df_interpolated['sat'] = 'l8'\n",
    "s2df_interpolated['sat'] = 's2'\n",
    "optical = pd.concat([l8df_interpolated, s2df_interpolated]).sort_index()\n",
    "optical = optical.loc[~optical.index.duplicated(keep='last')] # when both s2 and l8 are present, keep s2\n",
    "optical.rename({'water_area_corrected': 'area'}, axis=1, inplace=True)\n",
    "sar = sar.rename({'sarea': 'area'}, axis=1)\n",
    "\n",
    "# We need to define the nominal area of the reservoir. It can be a close approximation\n",
    "NOMINAL_AREA = 200\n",
    "AREA_DEVIATION_THRESHOLD = 0.05 * 200   # ~5% of the nominal reservoir area works well (Das et al., 2022)\n",
    "\n",
    "# Apply the trend based corrections\n",
    "result = trend_based_correction(optical.copy(), sar.copy(), AREA_DEVIATION_THRESHOLD)\n",
    "\n",
    "# Save the result\n",
    "result.reset_index().rename({\"index\": \"date\"}, axis=1).to_csv(\"../data/tms-os-sirindhorn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(                               # Optical sensors\n",
    "    x = optical.index,\n",
    "    y = optical['water_area'],\n",
    "    mode = 'markers+lines',\n",
    "    name = 'Surface Area: Optical',\n",
    "    opacity=0.7\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(                               # SAR \n",
    "    x = sar.index,\n",
    "    y = sar['area'],\n",
    "    mode = 'markers+lines',\n",
    "    name = 'Surface Area: SAR',\n",
    "    opacity=0.7\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(                               # TMS-OS\n",
    "    x = result.index,\n",
    "    y = result['filled_area'],\n",
    "    mode = 'markers+lines',\n",
    "    name = 'Surface Area: TMS-OS'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(                                         # title\n",
    "        text='Reservoir Surface Areas - TMS-OS',\n",
    "        xanchor='center',\n",
    "        x=0.5\n",
    "    ),\n",
    "    legend=dict(                                        # legend\n",
    "        orientation = 'h',\n",
    "        yanchor='top',\n",
    "        y=-0.08,\n",
    "        xanchor='right',\n",
    "        x=1.0,\n",
    "        bordercolor=\"grey\",\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    margin=dict(l=20, r=20, t=60, b=20)                 # margins\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c300dfce7f130addde34bb35dc5597fc44914ad651cf339f2e6729ce57523eb4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
